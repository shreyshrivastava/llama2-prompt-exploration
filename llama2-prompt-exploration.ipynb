{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "535a6303acc54ec7bd5e12809b93af82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32b6df00e4e149cfb95bbcb6859a99fe",
              "IPY_MODEL_6462d66064d747d680f8f22b1466c565",
              "IPY_MODEL_e976b3da74af4811b9f91b728e2b043f"
            ],
            "layout": "IPY_MODEL_0e301f2a3642436fbd69591110e4fbf3"
          }
        },
        "32b6df00e4e149cfb95bbcb6859a99fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_061a3f9bb852447486b0cec257cdb43f",
            "placeholder": "​",
            "style": "IPY_MODEL_f11975bb1e294726922dc3514fe8680c",
            "value": "llama-2-13b-chat.ggmlv3.q5_1.bin: 100%"
          }
        },
        "6462d66064d747d680f8f22b1466c565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e99d9a64a2eb4a7d9ed4786bebfe1ebd",
            "max": 9763701888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2444cd9570654935bcd560d7ab5b754d",
            "value": 9763701888
          }
        },
        "e976b3da74af4811b9f91b728e2b043f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084cead372804138a7cecffac4115044",
            "placeholder": "​",
            "style": "IPY_MODEL_2ca4a1d729f24cbb9039ca6278748b0a",
            "value": " 9.76G/9.76G [01:13&lt;00:00, 187MB/s]"
          }
        },
        "0e301f2a3642436fbd69591110e4fbf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "061a3f9bb852447486b0cec257cdb43f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f11975bb1e294726922dc3514fe8680c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e99d9a64a2eb4a7d9ed4786bebfe1ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2444cd9570654935bcd560d7ab5b754d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "084cead372804138a7cecffac4115044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca4a1d729f24cbb9039ca6278748b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Lab 12: Prompt Engineering**\n",
        "\n",
        "In this lab we are going to gain some practice with using LLM assistants and with prompt engineering.\n",
        "\n",
        "We will be do this with Llama2, an open source LLM, and  the llama2.cpp interface. You will have to write prompts to carry out several NLP tasks studied in the module.\n",
        "\n",
        "It is recommended you try to connect to a T4 GPU on Colab as this will speed up things considerably.\n",
        "\n",
        "The part of the  lab dedicated to setting up the interface is based on the HuggingFace lab https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/discussions/3."
      ],
      "metadata": {
        "id": "E3HqYZdfA0oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Llama 2"
      ],
      "metadata": {
        "id": "4bKQIsIq-d8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases. In this lab we will be using Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"
      ],
      "metadata": {
        "id": "PnV5UC7A2vBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Llama2.cpp\n",
        "\n",
        "`llama.cpp` can be used to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries."
      ],
      "metadata": {
        "id": "uAXhw_WHrXip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 0: Please read the description of the library at https://llama-cpp-python.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "bdr1W8P2em3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up llama cpp python"
      ],
      "metadata": {
        "id": "ubyTJi01e47w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The library works the same with a CPU, but the inference can take about three times longer compared to using it on a GPU.\n",
        "\n",
        "If you want to use only the CPU, you can replace the content of the cell below with the following lines.\n",
        "```\n",
        "# CPU llama-cpp-python\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "```"
      ],
      "metadata": {
        "id": "odwxcng9r_S8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkBmY3vQvRSw",
        "outputId": "981465b5-2005-49f3-d7c1-264917911c23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting setuptools>=42\n",
            "    Downloading setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 821.5/821.5 kB 9.0 MB/s eta 0:00:00\n",
            "  Collecting scikit-build>=0.13\n",
            "    Downloading scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.3/84.3 kB 9.6 MB/s eta 0:00:00\n",
            "  Collecting cmake>=3.18\n",
            "    Downloading cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.6/26.6 MB 56.3 MB/s eta 0:00:00\n",
            "  Collecting ninja\n",
            "    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 36.9 MB/s eta 0:00:00\n",
            "  Collecting distro (from scikit-build>=0.13)\n",
            "    Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "  Collecting packaging (from scikit-build>=0.13)\n",
            "    Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 7.6 MB/s eta 0:00:00\n",
            "  Collecting tomli (from scikit-build>=0.13)\n",
            "    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
            "    Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.8/65.8 kB 9.9 MB/s eta 0:00:00\n",
            "  Installing collected packages: ninja, wheel, tomli, setuptools, packaging, distro, cmake, scikit-build\n",
            "    Creating /tmp/pip-build-env-cx2h9ey3/overlay/local/bin\n",
            "    changing mode of /tmp/pip-build-env-cx2h9ey3/overlay/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-cx2h9ey3/overlay/local/bin/wheel to 755\n",
            "    changing mode of /tmp/pip-build-env-cx2h9ey3/overlay/local/bin/distro to 755\n",
            "    changing mode of /tmp/pip-build-env-cx2h9ey3/overlay/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-cx2h9ey3/overlay/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-cx2h9ey3/overlay/local/bin/ctest to 755\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "  Successfully installed cmake-3.29.0.1 distro-1.9.0 ninja-1.11.1.1 packaging-24.0 scikit-build-0.17.6 setuptools-69.2.0 tomli-2.0.1 wheel-0.43.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python.egg-info\n",
            "  writing /tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-pbqzx7sl/llama_cpp_python-0.1.78.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.1.78)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m174.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "\n",
            "\n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Ninja' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "\n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Configuring done (0.9s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_cmake_test_compile/build\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Ninja' generator - success\n",
            "  --------------------------------------------------------------------------------\n",
            "\n",
            "  Configuring Project\n",
            "    Working directory:\n",
            "      /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-build\n",
            "    Command:\n",
            "      /tmp/pip-build-env-cx2h9ey3/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-cx2h9ey3/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-cx2h9ey3/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-cx2h9ey3/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
            "    Git repository not found; to enable automatic generation of build info,\n",
            "    make sure Git is installed and the project is a Git repository.\n",
            "\n",
            "\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Configuring done (3.8s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-build\n",
            "  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
            "  [3/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  [4/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  [6/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so\n",
            "  [7/9] Linking CXX shared library vendor/llama.cpp/libllama.so\n",
            "  [8/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a\n",
            "  [8/9] Install the project...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n",
            "  -- Installing: /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\" to \"\"\n",
            "\n",
            "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n",
            "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n",
            "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n",
            "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n",
            "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n",
            "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n",
            "  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n",
            "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n",
            "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n",
            "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n",
            "  copying /tmp/pip-install-_ac3bix2/llama-cpp-python_5de4895886d54ba69143466a86161b0c/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copied 9 files\n",
            "  running build_ext\n",
            "  installing to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copied 11 files\n",
            "  running install_data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  copied 0 files\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-x3dgk1bl/.tmp-t33mww0a/llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'llama_cpp/__init__.py'\n",
            "  adding 'llama_cpp/libllama.so'\n",
            "  adding 'llama_cpp/llama.py'\n",
            "  adding 'llama_cpp/llama_cpp.py'\n",
            "  adding 'llama_cpp/llama_grammar.py'\n",
            "  adding 'llama_cpp/llama_types.py'\n",
            "  adding 'llama_cpp/py.typed'\n",
            "  adding 'llama_cpp/utils.py'\n",
            "  adding 'llama_cpp/server/__init__.py'\n",
            "  adding 'llama_cpp/server/__main__.py'\n",
            "  adding 'llama_cpp/server/app.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n",
            "  removing _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5811129 sha256=87a6c994b79d3878b8912bd6c22568d338bfe2ebd8ab0bbdbaa8549a059459fb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z7bwrvz9/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.10.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/bin/f2py3\n",
            "      Removing file or directory /usr/local/bin/f2py3.10\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.25.2.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.26.4 typing-extensions-4.11.0\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_nBHTYSoIWV",
        "outputId": "817ac62d-81c9-42a4-82f7-ab39b83c3028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "# To download the models\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R11KqY7lW0yv"
      },
      "source": [
        "# Choosing the Llama2 version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzJkCoRRbICP"
      },
      "source": [
        "\n",
        "Next, we need to specify which version of Llama2 to use. In Colab with T4 GPU, we can run models of up to 20B of parameters with all optimizations, but this may degrade the quality of the model's inference. The library can run GGML models on a CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHAz1yGZb4lq"
      },
      "source": [
        "In this lab, we will use  [Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)\n",
        "\n",
        "![asd](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc24dac6d-6b5e-4b5f-938c-05951c938a9e_1085x543.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSeedwAFSay9"
      },
      "source": [
        "#  Quantized Models from the Hugging Face Community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QwfjGFYRM4g"
      },
      "source": [
        "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
        "\n",
        "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
        "\n",
        "We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n",
        "\n",
        "\n",
        "\n",
        "In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prefix 'q5_1' signifies the quantization method we used. To determine the best method in each case, one rule is that 'q8' yields superior responses at the cost of higher memory usage [slow]. On the other hand, 'q2' may generate subpar responses but requires less RAM [fast].\n",
        "\n",
        "There are other quantization methods available, and you can read about them in the [model card](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)"
      ],
      "metadata": {
        "id": "4L7AZFe_7Px0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI-kXwg5bHF-"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Td05XSuiWdI"
      },
      "source": [
        "We download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "535a6303acc54ec7bd5e12809b93af82",
            "32b6df00e4e149cfb95bbcb6859a99fe",
            "6462d66064d747d680f8f22b1466c565",
            "e976b3da74af4811b9f91b728e2b043f",
            "0e301f2a3642436fbd69591110e4fbf3",
            "061a3f9bb852447486b0cec257cdb43f",
            "f11975bb1e294726922dc3514fe8680c",
            "e99d9a64a2eb4a7d9ed4786bebfe1ebd",
            "2444cd9570654935bcd560d7ab5b754d",
            "084cead372804138a7cecffac4115044",
            "2ca4a1d729f24cbb9039ca6278748b0a"
          ]
        },
        "id": "cBEJr-G-2ht4",
        "outputId": "ae297dd9-6c1c-42e3-96bd-db78b03b5079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-13b-chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "535a6303acc54ec7bd5e12809b93af82"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "max1jwxvCSbm"
      },
      "source": [
        "# Inference with llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TOfnZpj394g"
      },
      "source": [
        "Setting up the interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78533504-7fe4-40b4-d3d9-374a0bea839c",
        "id": "I7BvKO0lv_Wc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "from llama_cpp import Llama\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=4096, # Context window\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To run in CPU\n",
        "```\n",
        "# CPU\n",
        "from llama_cpp import Llama\n",
        "\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UdZnPtB8-Bhx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeH6eWiKuaxW",
        "outputId": "1d9631ed-0fc1-4071-9e6c-f567de2a4be3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# See the number of layers in GPU\n",
        "lcpp_llm.params.n_gpu_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First example of prompt use: generating code"
      ],
      "metadata": {
        "id": "KFqumvkEm3KJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLEEOufGVlID"
      },
      "source": [
        "A zero shot prompt asking Llama2 to write linear regression code in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NzVIlMCVoVD"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a linear regression in python\"\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaFQjPhcFgfN"
      },
      "source": [
        "Generating response\n",
        "\n",
        "If you only use CPU, the response can take a long time. You can reduce the max_tokens to get a faster response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R76uxL293jTc",
        "outputId": "3ba4e8a5-9a56-4b22-e5ae-7cff5a0ff1f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
            "\n",
            "USER: Write a linear regression in python\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "To write a linear regression in Python, you can use the scikit-learn library. Here is an example of how to do this:\n",
            "```\n",
            "from sklearn.linear_model import LinearRegression\n",
            "import pandas as pd\n",
            "\n",
            "# Load your dataset into a Pandas DataFrame\n",
            "df = pd.read_csv('your_data.csv')\n",
            "\n",
            "# Create a linear regression object and fit the data\n",
            "reg = LinearRegression()\n",
            "reg.fit(df[['x1', 'x2']], df['y'])\n",
            "\n",
            "# Print the coefficients\n",
            "print(reg.coef_)\n",
            "```\n",
            "This code will load your dataset into a Pandas DataFrame, create a linear regression object, and fit the data to the model using the `fit()` method. The `coef_` attribute of the `LinearRegression` object contains the estimated coefficients of the linear regression.\n",
            "\n",
            "You can also use the `predict()` method to make predictions on new data:\n",
            "```\n",
            "# Create a new DataFrame with predicted values\n",
            "preds = reg.predict(df[['x1', 'x2']])\n",
            "print(preds)\n",
            "```\n",
            "This code will create a new DataFrame with the\n"
          ]
        }
      ],
      "source": [
        "response = lcpp_llm(\n",
        "    prompt=prompt_template,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second example (also zero shot): a natural language generation task"
      ],
      "metadata": {
        "id": "MpqTD_Ioj4FN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlZk9uaqnW7x"
      },
      "source": [
        "A zero shot prompt asking Llama2 to write a story\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey6xRNFMnixL"
      },
      "outputs": [],
      "source": [
        "prompt_nlg = \"Write a story about a bear called Paddington\"\n",
        "prompt_template_nlg=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt_nlg}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "771ae635-24be-4c01-96ca-326c77506041",
        "id": "5ERhRzjXn8Vt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
            "\n",
            "USER: Write a story about a bear called Paddington\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "Once upon a time in Peru, there was a little bear named Paddington. He lived with his Aunt Lucy in the heart of the forest. Paddington loved to explore and play in the trees, but he always made sure to be back home for tea time. One day, a kind old man named Mr. Brown found Paddington lost in London. He took him home to his family and they all fell in love with the little bear's charming ways. From then on, Paddington lived with the Browns and had many exciting adventures with them. Despite being far from his forest home, Paddington always remained a curious and loving bear.\n",
            "\n",
            "Would you like me to add anything else?\n"
          ]
        }
      ],
      "source": [
        "response_nlg = lcpp_llm(\n",
        "    prompt=prompt_template_nlg,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response_nlg[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: natural language generation with zero-shot prompting  \n",
        "\n"
      ],
      "metadata": {
        "id": "aElKimJpZn7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Write a prompt to get Llama2 to generate a recipe for spaghetti al pomodoro. Experiment with different prompts."
      ],
      "metadata": {
        "id": "nlCHmJ1RcLTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write your prompt below, and call it ```prompt_task1```"
      ],
      "metadata": {
        "id": "-tF0K77AjWKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_task1 = # YOUR PROMPT HERE\n",
        "prompt_template_task1 = f'''\n",
        "\n",
        "USER: {prompt_task1}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "xDCOTUjHcc0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c056a4e1-8aa7-4d1f-8c4b-df2859e21288",
        "id": "fam0IgsbeHMO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "USER: Write a recipe for spaghetti al pomodoro\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "Spaghetti al Pomodoro is a classic Italian dish that combines the simplicity of pasta with the rich flavors of tomato and basil. Here's a simple recipe to make this delicious dish at home:\n",
            "\n",
            "Ingredients:\n",
            "\n",
            "* 12 oz spaghetti\n",
            "* 3 large tomatoes, seeded and chopped\n",
            "* 4 garlic cloves, minced\n",
            "* 1/4 cup extra-virgin olive oil\n",
            "* 2 tbsp fresh basil leaves, chopped\n",
            "* Salt and black pepper to taste\n",
            "* Grated Parmesan cheese for serving (optional)\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Bring a large pot of salted water to boil. Cook the spaghetti according to package instructions until al dente. Reserve 1 cup of pasta cooking water before draining the spaghetti.\n",
            "2. In a blender or food processor, combine tomatoes, garlic, and olive oil. Blend until smooth and creamy.\n",
            "3. Heat the blended sauce in a large skillet over medium heat for about 5 minutes, stirring\n"
          ]
        }
      ],
      "source": [
        "response_task1 = lcpp_llm(\n",
        "    prompt=prompt_template_task1,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response_task1[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: summarization with zero-shot prompting  \n",
        "\n"
      ],
      "metadata": {
        "id": "fIK0wzcEfj_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Write a prompt to get llama2 to produce a summary of the following Wikipedia article on Llama"
      ],
      "metadata": {
        "id": "fa_5ceCffrjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task2_article = f'''\n",
        "LLaMA\n",
        "\n",
        "LLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs), released by Meta AI starting in February 2023.\n",
        "\n",
        "For the first version of LLaMA, four model sizes were trained: 7, 13, 33, and 65 billion parameters.\n",
        "LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]\n",
        "\n",
        "In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters.\n",
        "\n",
        "LLaMA-2\n",
        "\n",
        "On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA.\n",
        "Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.[4]\n",
        "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[5]\n",
        "The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n",
        "\n",
        "LLaMA-2 includes both foundational models and models fine-tuned for dialog, called LLaMA-2 Chat.\n",
        "In further departure from LLaMA-1, all models are released with weights, and are free for many commercial use cases.\n",
        "However, due to some remaining restrictions, the description of LLaMA as open source has been disputed by the Open Source Initiative\n",
        "(known for maintaining the Open Source Definition).[6]\n",
        "\n",
        "Architecture\n",
        "\n",
        "LLaMA uses the transformer architecture, the standard architecture for language modeling since 2018.\n",
        "\n",
        "There are minor architectural differences. Compared to GPT-3, LLaMA\n",
        "\n",
        "- uses SwiGLU[7] activation function instead of GeLU;\n",
        "- uses rotary positional embeddings[8] instead of absolute positional embedding;\n",
        "- uses root-mean-squared layer-normalization[9] instead of standard layer-normalization.[10]\n",
        "- increases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\n",
        "\n",
        "Training datasets\n",
        "\n",
        "LLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process.\n",
        "\n",
        "LLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[1]\n",
        "\n",
        "-     Webpages scraped by CommonCrawl\n",
        "-     Open source repositories of source code from GitHub\n",
        "-     Wikipedia in 20 different languages\n",
        "-     Public domain books from Project Gutenberg\n",
        "-     The LaTeX source code for scientific papers uploaded to ArXiv\n",
        "-     Questions and answers from Stack Exchange websites\n",
        "\n",
        "Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[5] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\n",
        "'''"
      ],
      "metadata": {
        "id": "H3nXTPBWf1Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write your prompt below, and call it ```prompt_task2```"
      ],
      "metadata": {
        "id": "Z6Q6LAYhjJZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_task2 = # YOUR PROMPT HERE\n",
        "\n",
        "prompt_template_task2 = f'''\n",
        "\n",
        "USER: {prompt_task2}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "OdD7vsOQhYNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e676a830-dcb1-486f-f2a1-36dcefdb6869",
        "id": "lhQXws0Aigaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "USER: Summarize the Wikipedia article you find in the following lines, beginning with ARTICLE-BEGIN and ending with ARTICLE-NEXT\n",
            "ARTICLE-BEGIN \n",
            "\n",
            "LLaMA\n",
            "\n",
            "LLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs), released by Meta AI starting in February 2023.\n",
            "\n",
            "For the first version of LLaMA, four model sizes were trained: 7, 13, 33, and 65 billion parameters. \n",
            "LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]\n",
            "\n",
            "In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters.\n",
            "\n",
            "LLaMA-2\n",
            "\n",
            "On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA. \n",
            "Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.[4] \n",
            "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[5] \n",
            "The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n",
            "\n",
            "LLaMA-2 includes both foundational models and models fine-tuned for dialog, called LLaMA-2 Chat. \n",
            "In further departure from LLaMA-1, all models are released with weights, and are free for many commercial use cases. \n",
            "However, due to some remaining restrictions, the description of LLaMA as open source has been disputed by the Open Source Initiative \n",
            "(known for maintaining the Open Source Definition).[6]\n",
            "\n",
            "Architecture\n",
            "\n",
            "LLaMA uses the transformer architecture, the standard architecture for language modeling since 2018.\n",
            "\n",
            "There are minor architectural differences. Compared to GPT-3, LLaMA\n",
            "\n",
            "- uses SwiGLU[7] activation function instead of GeLU;\n",
            "- uses rotary positional embeddings[8] instead of absolute positional embedding;\n",
            "- uses root-mean-squared layer-normalization[9] instead of standard layer-normalization.[10]\n",
            "- increases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\n",
            "\n",
            "Training datasets\n",
            "\n",
            "LLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process.\n",
            "\n",
            "LLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[1]\n",
            "\n",
            "-     Webpages scraped by CommonCrawl\n",
            "-     Open source repositories of source code from GitHub\n",
            "-     Wikipedia in 20 different languages\n",
            "-     Public domain books from Project Gutenberg\n",
            "-     The LaTeX source code for scientific papers uploaded to ArXiv\n",
            "-     Questions and answers from Stack Exchange websites\n",
            "\n",
            "Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[5] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\n",
            "\n",
            "ARTICLE-END\n",
            "\n",
            "\n",
            "ASSISTANT:\n",
            "Here is a summary of the article you provided:\n",
            "\n",
            "LLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs) released by Meta AI starting in February 2023. The model has been trained on a large dataset of web pages, open source repositories, Wikipedia articles, and other publicly available data sources. LLaMA-2 was released in July 2023 with improved performance and additional fine-tuning for dialogue. The models are available under a noncommercial license and can be used for many commercial use cases. However, due to some remaining restrictions, the description of LLaMA as open source has been disputed by the Open Source Initiative.\n",
            "\n",
            "Is there anything else you would like me to assist with?\n"
          ]
        }
      ],
      "source": [
        "response_task2 = lcpp_llm(\n",
        "    prompt=prompt_template_task2,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response_task2[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: machine translation with zero-shot prompting  \n",
        "\n"
      ],
      "metadata": {
        "id": "lbHAysAbjjnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Write a prompt to get llama2 to translate the Wikipedia article above to French if you are English, else to your native language"
      ],
      "metadata": {
        "id": "NtzFDZ5OnV90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write your prompt below, and call it ```prompt_task3```"
      ],
      "metadata": {
        "id": "Msbu8qvKnoRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_task3 = # YOUR PROMPT HERE\n",
        "prompt_template_task3 = f'''\n",
        "\n",
        "USER: {prompt_task3}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "kIRTWZLbnwkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add959e6-e892-47a7-dd3a-3f6898d1bb19",
        "id": "b8dqBVRloGgq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "USER: Translate to Italian the Wikipedia article you find in the following lines, beginning with ARTICLE-BEGIN and ending with ARTICLE-NEXT\n",
            "ARTICLE-BEGIN\n",
            "\n",
            "LLaMA\n",
            "\n",
            "LLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs), released by Meta AI starting in February 2023.\n",
            "\n",
            "For the first version of LLaMA, four model sizes were trained: 7, 13, 33, and 65 billion parameters. \n",
            "LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]\n",
            "\n",
            "In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters.\n",
            "\n",
            "LLaMA-2\n",
            "\n",
            "On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA. \n",
            "Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.[4] \n",
            "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[5] \n",
            "The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n",
            "\n",
            "LLaMA-2 includes both foundational models and models fine-tuned for dialog, called LLaMA-2 Chat. \n",
            "In further departure from LLaMA-1, all models are released with weights, and are free for many commercial use cases. \n",
            "However, due to some remaining restrictions, the description of LLaMA as open source has been disputed by the Open Source Initiative \n",
            "(known for maintaining the Open Source Definition).[6]\n",
            "\n",
            "Architecture\n",
            "\n",
            "LLaMA uses the transformer architecture, the standard architecture for language modeling since 2018.\n",
            "\n",
            "There are minor architectural differences. Compared to GPT-3, LLaMA\n",
            "\n",
            "- uses SwiGLU[7] activation function instead of GeLU;\n",
            "- uses rotary positional embeddings[8] instead of absolute positional embedding;\n",
            "- uses root-mean-squared layer-normalization[9] instead of standard layer-normalization.[10]\n",
            "- increases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\n",
            "\n",
            "Training datasets\n",
            "\n",
            "LLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process.\n",
            "\n",
            "LLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[1]\n",
            "\n",
            "-     Webpages scraped by CommonCrawl\n",
            "-     Open source repositories of source code from GitHub\n",
            "-     Wikipedia in 20 different languages\n",
            "-     Public domain books from Project Gutenberg\n",
            "-     The LaTeX source code for scientific papers uploaded to ArXiv\n",
            "-     Questions and answers from Stack Exchange websites\n",
            "\n",
            "Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[5] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\n",
            "\n",
            "ARTICLE-END\n",
            "\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "The article you have provided is in English, but I can certainly help you translate it into Italian! Here's the translation of the text you provided, starting from ARTICLE-BEGIN and ending at ARTICLE-NEXT:\n",
            "\n",
            "ARTICOLO-INIZIO\n",
            "\n",
            "LLaMA (Large Language Model Meta AI) è una famiglia di modelli di linguaggio liberi autoregressivi (LLMs), messi a disposizione da Meta AI a partire da febbraio 2023.\n",
            "\n",
            "Per la prima versione di LLaMA, sono state allenate quattro taglie di parametri: 7, 13, 33 e 65 miliardi. I sviluppatori di LLaMA hanno riferito che il modello con 13 miliardi di parameter ha eseguito migliori prestazioni su quasi tutti i benchmark NLP rispetto al molto più grande GPT-3 (con 175 miliardi di parametri) e che il modello più grande era competitivo con i modelli di stato dell'arte come PaLM e Chinchilla.[1]\n",
            "\n",
            "In luglio 20\n"
          ]
        }
      ],
      "source": [
        "response_task3 = lcpp_llm(\n",
        "    prompt=prompt_template_task3,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response_task3[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: named entity recognition, one and few-shot prompting, JSON output\n",
        "\n"
      ],
      "metadata": {
        "id": "0c51-BocoX5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Write a prompt to get llama2 to tag named entities in the following sentence as 'ORG' if organization, 'DATE' if date, 'NUM' if number, and 'MODEL' if an AI model, and to output the result in JSON format. Use a few examples to explain llama2 what you want."
      ],
      "metadata": {
        "id": "yRt_bLDcpOOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task4_sentence = \"On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA. Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.\""
      ],
      "metadata": {
        "id": "J44m7lsZpdlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write your prompt below, and call it ```prompt_task4```"
      ],
      "metadata": {
        "id": "d7K1KYhSrRjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_task4 = # YOUR PROMPT HERE\n",
        "prompt_template_task4 = f'''\n",
        "\n",
        "USER: {prompt_task4}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "r83XLDIMrX7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1b0be8-3442-4425-9015-e4be0b2c5dce",
        "id": "9q_vkes4sdE_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "USER: Identify the named entities in the sentence in the following lines (between with SENTENCE-BEGIN and SENTENCE-END), and tag them as\n",
            "ORG if organization (for instance, Apple, IBM)\n",
            "DATE if date (for instance, 8/4/2024)\n",
            "NUM if number (for instance, 65)\n",
            "MODEL if an AI model (for instance, ChatGPT)\n",
            "Output the result in JSON format.\n",
            "\n",
            "SENTENCE-BEGIN\n",
            "On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA. Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.\n",
            "SENTENCE-END\n",
            "\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "{\n",
            "\"entities\": [\n",
            "{\n",
            "\"type\": \"DATE\",\n",
            "\"value\": \"July 18, 2023\"\n",
            "},\n",
            "{\n",
            "\"type\": \"ORG\",\n",
            "\"value\": \"Meta\"\n",
            "},\n",
            "{\n",
            "\"type\": \"NUM\",\n",
            "\"value\": \"7, 13, and 70 billion parameters\"\n",
            "}\n",
            "]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "response_task4 = lcpp_llm(\n",
        "    prompt=prompt_template_task4,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response_task4[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did Llama2 do at the task? Try to experiment with both one-shot and few-shot prompts."
      ],
      "metadata": {
        "id": "T6F0xoUTs8eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: dialogue act tagging, one and few-shot prompting\n",
        "\n"
      ],
      "metadata": {
        "id": "HfYXQkG1tO1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Write a prompt to get llama2 to tag dialogue acts in the following conversation, using your favourite dialogue act tagset, and to output the result in JSON format. Use a few examples to explain llama2 what you want."
      ],
      "metadata": {
        "id": "nIRnQMGYtX01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task5_conversation = f'''\n",
        "A: . . . I need to travel in May.\n",
        "B: And, what day in May did you want to travel?\n",
        "A: OK uh I need to be there for a meeting that’s from the 12th to the 15th.\n",
        "B: And you’re flying into what city?\n",
        "A: Seattle.\n",
        "B: And what time would you like to leave Pittsburgh?\n",
        "A: Uh hmm I don’t think there’s many options for non-stop.\n",
        "B: Right. There’s three non-stops today.\n",
        "'''"
      ],
      "metadata": {
        "id": "1o2M0ofottvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write your prompt below, and call it ```prompt_task5```"
      ],
      "metadata": {
        "id": "QTyTJP58uXL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_task5 = # YOUR PROMPT HERE\n",
        "prompt_template_task5 = f'''\n",
        "\n",
        "USER: {prompt_task5}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "GZhJ1lv5uhNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66695038-27f8-422d-ab69-a511185479fb",
        "id": "ljdXu4kJvzjx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "USER: Identify the dialogue acts in the conversation below between A and B (starting from CONVERSATION-BEGIN and ending with CONVERSATION-END), using the following tags:\n",
            "QW if the utterance is a wh-question (for instance, where are you going?)\n",
            "SD if the utterance is a statement (for instance, I am going to London)\n",
            "ACK if the utterance is an acknowledgment (for instance, OK)\n",
            "Output the result in JSON format.\n",
            "\n",
            "CONVERSATION-BEGIN\n",
            "\n",
            "A: . . . I need to travel in May.\n",
            "B: And, what day in May did you want to travel?\n",
            "A: OK uh I need to be there for a meeting that’s from the 12th to the 15th.\n",
            "B: And you’re flying into what city?\n",
            "A: Seattle.\n",
            "B: And what time would you like to leave Pittsburgh?\n",
            "A: Uh hmm I don’t think there’s many options for non-stop.\n",
            "B: Right. There’s three non-stops today.\n",
            "\n",
            "CONVERSATION-END\n",
            "\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "{\n",
            "\"acts\": [\n",
            "{\n",
            "\"type\": \"QW\",\n",
            "\"utterance\": \"And, what day in May did you want to travel?\"\n",
            "},\n",
            "{\n",
            "\"type\": \"SD\",\n",
            "\"utterance\": \"OK uh I need to be there for a meeting that’s from the 12th to the 15th.\"\n",
            "},\n",
            "{\n",
            "\"type\": \"ACK\",\n",
            "\"utterance\": \"And you’re flying into what city?\"\n",
            "},\n",
            "{\n",
            "\"type\": \"SD\",\n",
            "\"utterance\": \"Seattle.\"\n",
            "},\n",
            "{\n",
            "\"type\": \"QW\",\n",
            "\"utterance\": \"And what time would you like to leave Pittsburgh?\"\n",
            "},\n",
            "{\n",
            "\"type\": \"ACK\",\n",
            "\"utterance\": \"Uh hmm I don’t think there’s many options for non-stop.\"\n",
            "},\n",
            "{\n",
            "\"type\": \"SD\",\n",
            "\"utterance\": \"Right. There’s three non-stops today.\"\n",
            "}\n",
            "]\n",
            "}\n",
            "\n",
            "Please note that the above conversation is just an example, and the actual dialogue acts may vary based on the specific context of the conversation.\n"
          ]
        }
      ],
      "source": [
        "response_task5 = lcpp_llm(\n",
        "    prompt=prompt_template_task5,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response_task5[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did llama2 do?"
      ],
      "metadata": {
        "id": "O8-kbvBzwD7-"
      }
    }
  ]
}